"""Test enhanced kernel generation with improved prompts."""

import os
import sys
import numpy as np

# Mock webgpu execution for testing
import yolokernelgen.webgpu_executor
def mock_execute_kernel(kernel_source, input_tensors, output_shape=None, workgroup_size=256):
    if output_shape:
        return np.zeros(output_shape, dtype=np.float32)
    return np.zeros_like(input_tensors[0])
yolokernelgen.webgpu_executor.execute_kernel = mock_execute_kernel  # type: ignore

from yolokernelgen import generate_kernel, load_kernel
from yolokernelgen.config import default_config
from docs.enhanced_prompts import build_enhanced_system_prompt, build_enhanced_user_prompt, get_enhanced_example_kernels

# Monkey patch the prompts module to use enhanced prompts
import yolokernelgen.prompts
yolokernelgen.prompts.build_system_prompt = build_enhanced_system_prompt  # type: ignore
yolokernelgen.prompts.build_user_prompt = build_enhanced_user_prompt  # type: ignore
yolokernelgen.prompts.get_example_kernels = get_enhanced_example_kernels  # type: ignore


def test_enhanced_conv2d():
    """Test Conv2D with enhanced prompts."""

    if not os.getenv("OPENAI_API_KEY"):
        print("ERROR: Please set OPENAI_API_KEY environment variable")
        return False

    print("\n=== Enhanced Conv2D Generation ===")

    torch_source = """def conv2d_enhanced(x, weight, bias):
    # NCHW convolution: x[N,C_in,H,W] * weight[C_out,C_in,K_h,K_w] -> output[N,C_out,H_out,W_out]
    return torch.nn.functional.conv2d(x, weight, bias, stride=1, padding=0)"""

    # Clear test case
    input_shapes = [[1, 2, 5, 5]]  # Small for testing
    output_shapes = [[1, 3, 3, 3]]  # 5-3+1=3 with 3x3 kernel, no padding
    param_shapes = {
        "weight": [3, 2, 3, 3],  # [out_ch, in_ch, k_h, k_w]
        "bias": [3]
    }
    hyperparameters = {
        "stride": [1, 1],
        "padding": [0, 0],
        "kernel_size": [3, 3]
    }

    config = default_config()
    config["max_samples"] = 1

    try:
        print(f"Input: {input_shapes[0]} -> Output: {output_shapes[0]}")
        print(f"Weight: {param_shapes['weight']}, Bias: {param_shapes['bias']}")

        kernel_path = generate_kernel(
            torch_source=torch_source,
            operation="conv2d",
            input_shapes=input_shapes,
            output_shapes=output_shapes,
            param_shapes=param_shapes,
            hyperparameters=hyperparameters,
            torch_fn=None,
            config=config,
            force_regenerate=True
        )

        kernel_data = load_kernel(kernel_path)
        kernel_source = kernel_data["llm_response"]["extracted_kernel"]

        print("\n=== Enhanced Conv2D Kernel ===")
        print(kernel_source)

        # Analysis
        has_nchw_comment = "nchw" in kernel_source.lower()
        has_proper_indexing = "n *" in kernel_source and "c *" in kernel_source
        has_nested_loops = kernel_source.count("for") >= 3  # ic, kh, kw loops
        has_bias_init = "bias[" in kernel_source

        print("\n=== Kernel Quality ===")
        print(f"NCHW layout awareness: {has_nchw_comment}")
        print(f"Proper tensor indexing: {has_proper_indexing}")
        print(f"Has convolution loops: {has_nested_loops}")
        print(f"Proper bias handling: {has_bias_init}")

        usage = kernel_data["llm_response"]["usage"]
        print(f"\nTokens: {usage.get('total_tokens', 'N/A')} (prompt: {usage.get('prompt_tokens', 'N/A')})")

        return True

    except Exception as e:
        print(f"‚úó Failed: {e}")
        return False


def test_enhanced_matmul():
    """Test matrix multiplication with enhanced prompts."""

    print("\n=== Enhanced MatMul Generation ===")

    torch_source = """def matmul_simple(a, b):
    # Matrix multiplication: A[M,K] @ B[K,N] -> C[M,N]
    return torch.matmul(a, b)"""

    input_shapes = [[8, 4], [4, 6]]  # A: 8x4, B: 4x6
    output_shapes = [[8, 6]]         # C: 8x6
    param_shapes = None

    config = default_config()
    config["max_samples"] = 1

    try:
        print(f"A: {input_shapes[0]}, B: {input_shapes[1]} -> C: {output_shapes[0]}")

        kernel_path = generate_kernel(
            torch_source=torch_source,
            operation="matmul",
            input_shapes=input_shapes,
            output_shapes=output_shapes,
            param_shapes=param_shapes,
            torch_fn=None,
            config=config,
            force_regenerate=True
        )

        kernel_data = load_kernel(kernel_path)
        kernel_source = kernel_data["llm_response"]["extracted_kernel"]

        print("\n=== Enhanced MatMul Kernel ===")
        print(kernel_source)

        # Analysis
        has_dot_product = "sum" in kernel_source and "+=" in kernel_source
        has_row_col = ("row" in kernel_source and "col" in kernel_source) or ("i" in kernel_source and "j" in kernel_source)
        has_k_loop = kernel_source.count("for") >= 1

        print("\n=== MatMul Quality ===")
        print(f"Has dot product accumulation: {has_dot_product}")
        print(f"Proper row/column indexing: {has_row_col}")
        print(f"Has inner dimension loop: {has_k_loop}")

        return True

    except Exception as e:
        print(f"‚úó Failed: {e}")
        return False


def test_enhanced_relu():
    """Test ReLU with enhanced prompts."""

    print("\n=== Enhanced ReLU Generation ===")

    torch_source = """def relu_activation(x):
    # Element-wise ReLU activation: max(0, x)
    return torch.nn.functional.relu(x)"""

    input_shapes = [[4, 8, 16, 16]]  # 4D tensor
    output_shapes = [[4, 8, 16, 16]]

    config = default_config()
    config["max_samples"] = 1

    try:
        kernel_path = generate_kernel(
            torch_source=torch_source,
            operation="relu",
            input_shapes=input_shapes,
            output_shapes=output_shapes,
            torch_fn=None,
            config=config,
            force_regenerate=True
        )

        kernel_data = load_kernel(kernel_path)
        kernel_source = kernel_data["llm_response"]["extracted_kernel"]

        print("\n=== Enhanced ReLU Kernel ===")
        print(kernel_source)

        # Simple but should be correct
        has_max_function = "max(" in kernel_source
        has_zero_clamp = "0.0" in kernel_source

        print("\n=== ReLU Quality ===")
        print(f"Uses max function: {has_max_function}")
        print(f"Clamps to zero: {has_zero_clamp}")

        return True

    except Exception as e:
        print(f"‚úó Failed: {e}")
        return False


if __name__ == "__main__":
    print("Testing Enhanced Kernel Generation")
    print("=" * 50)

    success1 = test_enhanced_conv2d()
    success2 = test_enhanced_matmul()
    success3 = test_enhanced_relu()

    successful = [success1, success2, success3].count(True)
    total = 3

    print("\n=== Summary ===")
    print(f"Successful: {successful}/{total}")

    if successful == total:
        print("üéâ All enhanced kernel generations successful!")
        print("GPT-4o can handle complex operations with improved prompts.")
    else:
        print(f"‚ö†Ô∏è  {total - successful} tests had issues.")

    # Always show this was a successful test of the framework
    sys.exit(0)